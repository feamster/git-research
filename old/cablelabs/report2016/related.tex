\section{Related Data and Analysis Techniques}\label{sec:related}

In this section, we briefly outline existing attempts to measure both
end-to-end performance of Internet paths and infer
congestion along these paths (and at interconnects) using these
datasets.  
All of these techniques and approaches involve inference based on
measurements from end hosts, as opposed to direct measurements of
utilization at the interconnect. As a result, public data about
utilization and capacity at the interconnects---which this project
provides for the first time---fills a significant
gap concerning our visibility into the current state of utilization at
interconnects.  

\subsection{Measurements from End-Hosts}

A common approach to performing Internet performance measurements is to
actively send test traffic along end-to-end Internet paths and observe
the performance characteristics of those paths. For example, one might
perform test uploads or downloads from an end-user device (laptop,
phone, home gateway device) and measure the time to transfer a certain
number of bytes. Similarly, it is possible to measure end-to-end latency
or packet loss along these end-to-end paths, as well as to measure how
these characteristics may vary in response to additional load on the
network. 

\paragraph{Measurement Lab.} The Measurement Lab~\cite{www-mlab}
operates global server infrastructure for conducting throughput
measurements from various endpoints, using pre-approved measurement
tools. Measurement Lab (MLab)  limits the tools that can perform
throughput measurements against their servers due to the fact that
server bandwidth is a limited resource. One of the tools that has
permission to measure against this infrastructure is
BISmark~\cite{sundaresan2011,www-bismark} , which we 
describe in more detail below. Other tools for measuring mobile
performance (e.g., MobiPerf) exist. The tool that perhaps offers the
most comprehensive data from the project is the Network Diagnostic Tool
(NDT), which we also describe in more detail below.


\paragraph{Network Diagnostic Tool (NDT).} The network diagnostic tool (NDT)~\cite{www-ndt}
performs throughput tests; users run NDT from end-hosts,
which measure throughput to a corresponding server. One version of the
tool runs as a Java applet from a web browser. Measurement Lab runs a
version of the Java applet from its website that measures throughput to
the collection of deployed Measurement Lab servers around the world,
using geolocation to map the client to a nearby NDT server for the
purposes of the throughput test (the accuracy of a TCP throughput test
depends on measuring throughput to a nearby server, since TCP throughput
is inversely proportional to round-trip latency). 
NDT also forms the basis of well-known measurement efforts, such as the
Internet Health Test. 
Unfortunately, MLab's NDT
test setup is known to be inaccurate due to its use of only a single thread to
measure TCP throughput, which our previous work shows can significantly
underestimate the throughput of the link~\cite{sundaresan2011}. Additionally, NDT
provides no mechanism for locating a throughput bottleneck along an
end-to-end path.


\paragraph{BISmark.} The Broadband Internet Service Benchmark
(BISmark)~\cite{sundaresan2011,www-bismark} 
project runs custom throughput, latency, and packet loss measurements
from home routers that run
OpenWrt. The project has been
collecting 
performance data from home networks since 2011; at its peak, the project
was collecting data from about 400 home networks in more than 30
countries. Currently, about 70 home routers are actively reporting
measurements. The project was the first research effort to explore the
means of measuring access-network throughput and latency of a broadband
access network’s access link using direct measurements. All of the data
is publicly available, both through a web portal, and via direct
download in XML format. The BISmark measurement techniques perform
end-to-end measurements against deployed servers and do not attempt to
draw inferences or conclusions about congestion at interconnect. The
BISmark project produced the first published research paper that
documented interconnection congestion at many interconnects that
occurred in March 2014; because the measurements were end-to-end, they
manifested as pronounced increases in latency along specific end-to-end
paths between home Internet subscribers and the M-Lab
servers. Subsequent work, which we describe in the next section, has
followed up on this effort in more detail. 

\paragraph{FCC Measuring Broadband America Reports.} The FCC’s Measuring
Broadband America project~\cite{www-mba} produces periodic reports using similar measurements
as the BISmark project, albeit with a much larger deployment. Their
reports are less frequent (typically once per year), as opposed to
BISmark's ``real time'' visualizations of throughput, latency, and packet
loss. The techniques are similar (some of them, such as the
throughput test and the Web performance test, were co-designed), as are
the servers against which the home network gateways perform measurements
(i.e., both perform throughput measurements against the Measurement Lab
servers). Similarly, the project does not provide any mechanism for
directly measuring congestion at interconnection points; the only
performance measurements that the devices can perform are end-to-end
performance measurements. 

\subsection{Measuring Interconnect Performance}

Because the above tools can only measure from end-host vantage points,
they do not provide direct information about utilization or congestion
at interconnection points. Because congestion manifests as an
increase in latency, the measurement techniques that we have discussed
above can often detect congestion along an Internet path. Yet, detecting
congestion at a particular interconnection point is difficult to do with
these types of measurements. We discuss various other methods to
indirectly infer or directly measure congestion at an interconnection
point below.

\subsubsection{Indirect: Tomography \& Round-Trip Latency}

A general measurement approach sometimes referred to as {\em network
tomography} attempts to use a combination of performance measurements
along different end-to-end Internet paths to infer specific links where
congestion or failures may be occurring~\cite{coates2001network}. The intuition is quite simple:
Given ``simultaneous'' measurements of two end-to-end Internet paths that
may share one or more links, if one end-to-end path experiences symptoms
of congestion (i.e., an increase in latency) whereas a second end-to-end
path does not, then we can infer that the congestion must be occurring
on the portion of the second path that is not common with the first
path. One can generalize this to N end-to-end paths; the hope is there
is some set of end-to-end paths in the measurement infrastructure such
that each link could be isolated.  

Unfortunately, it is difficult to obtain a comprehensive set of vantage
points in practice because most end-to-end paths will share more than
one interconnection point or link in common. For example, in an M-Lab
report released in 2014~\cite{mlab-congestion}, many of the end-to-end paths between NDT
vantage points and the M-Lab servers could (and likely do) share
multiple end-to-end links along the path---not only the interconnection
point (where the report implies congestion is taking place) but also
other links along the path (e.g., links within transit providers).  The
second scenario is a
distinct possibility that previous reports have outlined in
detail~\cite{ftt-congestion-2015}, and it would be na\"{i}ve to suggest that
these 
measurements conclusively pinpoint congestion at interconnection
points. Worse yet, providers can (and have) gamed these active
measurement techniques by prioritizing probe traffic~\cite{kilmer2014}.

Another approach, proposed by CAIDA~\cite{www-caida}, is to use traceroutes to
discover an end-to-end 
path and subsequently send latency probes to either side of an
interconnect. While this approach is more direct than network
tomography, the approach entails significant shortcomings, which are
outlined in detail in CAIDA's own report. Among the limitations are the
difficulty in accurately identifying points of interconnection points
along and end-to-end traceroute, as well as the fact that increases in
latency might be occurring along reverse paths, as opposed to the
forward path that the probes are attempting to measure. 

\subsubsection{Direct: Packet Capture and SNMP}

An alternative method for directly gathering information about the traffic that
passes through a network is via packet traces.  Packet traces capture
what is effectively a recording of every packet that traverses a
particular interface. When packet trace collection is configured, an
administrator may capture the complete packets, the first bytes of each
packet, or simply the ``headers'' or metadata for each packet. Packet
capture provides complete timing information about the arrival of
individual packets and the header information on individual packets
(including the TCP window size), and can as such be used to compute or
infer properties of traffic flows including jitter, packet loss, and
instantaneous throughput.  

It would be beneficial to have better information about latency and
packet loss to assess the congestion status of a particular flow, as
well as what might be causing poor user quality of experience, but these
types of conclusions typically require gathering packet-level
statistics.  Methods such as deep-packet inspection are typically not
practical at large, high-throughput interconnection points; these
methods tend to be costly to deploy, and they produce more data than can
be reasonably backhauled to a data-center for post hoc analysis.
Additionally, typically gathering fine-grained, packet-level information
is not tenable at high packet rates, so gathering traffic flow
statistics must often suffice.  Traffic flow statistics are quite a bit
more coarse, because they only provide information about the number of
bytes and packets transferred over the duration of the
flow record.  Accordingly, although these types of methods may be
appropriate for certain types of analysis pertaining to quality of
experience, security, or other network management tasks, the question of
utilization of interconnects is best answered today with flow-level
statistics (e.g., IPFIX) or SNMP counters.  Traffic flow statistics can
represent traffic statistics on a per-flow basis as opposed to SNMP byte
counts, which only represent total interface utilization counts. SNMP
statistics are thus more coarse for many purposes, because they do not represent the
utilization of specific flows and are polled at relatively infrequent
intervals.

\paragraph{AT\&T/DirectTV Merger Analysis.} The Cooperative Association
for Internet Data Analysis~(CAIDA)~\cite{www-caida} is participating in an ongoing
consultation with the Federal Communications Commission~(FCC) concerning the
performance metrics that should be reported in conjunction with the
merger between AT\&T and DirectTV, to ensure that the network is
offering suitable performance to all traffic flows~\cite{fcc-att}.  Reporting on
performance at interconnection points is a condition of the
merger. The FCC appointed CAIDA as an independent measurement expert
(IME) to recommend a set of metrics that should be included in these
reports on interconnection performance; the recommendations suggest
including metrics such as packet loss, latency, and jitter of flows at
the interconnection point, as proxies for congestion. It is unclear at
this time what information, if anything, will be publicly reported. 

In contrast, this study only
reports on utilization, but these measurements are made public. Although
these recommendations suggest that 
utilization need not be a proxy for congestion (indeed, it may be
possible to engineer an interconnect to run at 95\% capacity or higher),
it is also worth noting that {\em none} of these metrics tell the
complete story. Similarly, packet loss may increase as a result of
active queue management or traffic shaping, as opposed to
congestion. Similarly, latency or jitter are only indirect proxies for
congestion.
